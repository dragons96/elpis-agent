# Define a model configuration called CHAT
# {MODEL_NAME}_BASE_URL is the base url of the model provider
CHAT_BASE_URL=https://api.openai.com/v1
# {MODEL_NAME}_API_KEY is the api key of the model provider, if not set, delete it
CHAT_API_KEY=
# {MODEL_NAME}_MODEL is the model name of the model provider
CHAT_MODEL=gpt4.1
# {MODEL_NAME}_MODEL_PROVIDER is the model provider name, allow: openai, ollama
CHAT_MODEL_PROVIDER=openai
# {MODEL_NAME}_MODEL_TYPE is the model type, allow: chat, embedding
CHAT_MODEL_TYPE=chat
# {MODEL_NAME}_TEMPERATURE is the temperature of the model provider
CHAT_TEMPERATURE=0.3

# Define a model configuration called EMBEDDING
EMBEDDING_BASE_URL=http://127.0.0.1:11434
EMBEDDING_MODEL=
EMBEDDING_MODEL_PROVIDER=ollama
EMBEDDING_MODEL_TYPE=embedding
EMBEDDING_TEMPERATURE=0.3

# Chat model using CHAT model configuration
CHAT_MODEL_KEY_PREFIX=CHAT
# The configuration of embedding models using EMBEDDING models, if not set, please delete it, codebase is unsed when embedding is not set.
EMBEDDING_MODEL_KEY_PREFIX=
#EMBEDDING_MODEL_KEY_PREFIX=EMBEDDING

# Custom system prompt
SYSTEM_PROMPT=

# Saver Checkpointer type, allow: [memory, sqlite], if not set, delete it and not used checkpointer.
CHECKPOINTER=memory

# MCP file path, default : ./mcp.json
# {
#   "mcpServers": {
#
#   }
# }
MCP_FILE_PATH=mcp.json

# Mem0 memory config, if you not set. its  will not used.
# By mem0 api
MEM0_API_KEY=
# Anothor mem0 config, Choose between MEM0_API_KEY and those.
# mem0 llm model, example: use named CHAT model.
MEM0_MODEL_KEY_PREFIX=CHAT
# mem0 embedding model, if not set, default use openai model and you should set OPENAI_API_KEY value. Example: use named EMBEDDING model.
MEM0_EMBEDDING_KEY_PREFIX=EMBEDDING
# mem0 vector store model dims, default is 1536
MEM0_VECTOR_STORE_EMBEDDING_MODEL_DIMS=1536
